{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hochschule Bonn-Rhein-Sieg\n",
    "\n",
    "# Mathematics for Robotics and Control, SS17\n",
    "\n",
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 1: Naive Bayes Classifier [40 points]\n",
    "\n",
    "The Naive Bayes classifier is a quite simple and popular classifier that is entirely based on a conditional independence assumption. Let's suppose that we have a set of $n$ features $F_1, F_2, ..., F_n$ and a class label $C$. The Naive Bayes classifier assumes that the features are independent of each other given a known class label, which is another way of saying that\n",
    "\n",
    "\\begin{equation*}\n",
    "    P(C, F_1, F_2, ..., F_n) = P(F_1, F_2, ..., F_n|C)P(C) = P(C)\\prod_{i}{P(F_i|C)}\n",
    "\\end{equation*}\n",
    "\n",
    "Now, the classification problem is to determine the class label given the values of the features $F_1, F_2, ..., F_n$. If we have $m$ classes, we can find the class label by calculating $P(C_j|F_1, F_2, ..., F_n)$ for $1 \\leq j \\leq m$\n",
    "\n",
    "\\begin{equation*}\n",
    "    P(C_j|F_1, F_2, ..., F_n) = \\frac{P(C_j, F_1, F_2, ..., F_n)}{P(F_1, F_2, ..., F_n)} = \\frac{P(C_j)\\prod_{i}{P(F_i|C_j)}}{P(F_1, F_2, ..., F_n)} = \\alpha P(C_j)\\prod_{i}{P(F_i|C_j)}\n",
    "\\end{equation*}\n",
    "\n",
    "and then choosing\n",
    "\n",
    "\\begin{equation*}\n",
    "    k = \\max_{j}P(C_j|F_1, F_2, ..., F_n)\n",
    "\\end{equation*}\n",
    "\n",
    "as the class label. In the above equation, we notice that $P(F_1, F_2, ..., F_n)$ doesn't have to be calculated explicitly, i.e. instead of calculating $P(F_1, F_2, ..., F_n)$, we can calculate $P(C_j)\\prod_{i}{P(F_i|C_j)}$ for $1 \\leq j \\leq m$ and then normalise the values using $\\alpha = \\sum_{j}{P(C_j)\\prod_{i}{P(F_i|C_j)}}$.\n",
    "\n",
    "In this exercise, you will implement your very own Naive Bayes classifier that can be used for predicting the stability of object placements on a table. The scenario is one in which our robot Jenny is putting objects on a table, such that we'll suppose that the robot chooses a random continuous table pose for placement and then tries to predict whether placing a *point object* there will be successful by describing the pose with a few features.\n",
    "\n",
    "The scenario we are considering is visualised from a top view in the image below (note: the blue square is the table, the red squares are the objects on it, and the orange dot is Jenny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkoAAAJtCAYAAADJil8XAAAABHNCSVQICAgIfAhkiAAADP5JREFU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\nAIJQAgAIQgkAIAglAIDwDwjluJ1GwmU0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.core.display.Image(\"images/configuration.png\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's suppose that a pose is described using the following features, all of which are discrete:\n",
    "\n",
    "* *Inside table*: Takes the values $0$ and $1$, corresponding to whether a pose is outside or inside the table respectively.\n",
    "* *Distance to the robot*: Takes the values $0$, $1$, and $2$, corresponding to *very close*, *reachable*, and *far*.\n",
    "* *Minimum distance to the other objects*: Takes the values $0$, $1$, and $2$, corresponding to *very close*, *close*, and *far*.\n",
    "* *Distance to the closest surface edge*: Also takes the values $0$, $1$, and $2$, corresponding to *very close*, *close*, and *far*.\n",
    "\n",
    "Each pose either leads to a successful execution or not, so we have two classes, namely $0$ and $1$, corresponding to the outcomes *failure* and *success* respectively.\n",
    "\n",
    "Your task now consists of the following steps:\n",
    "\n",
    "* You are given a data set (*data/train.txt*) of features describing $1500$ poses and the class labels of these. Use the data in this data set for learning the prior probabilities $P(C_j)$ and the conditional probabilities $P(F_i|C_j)$, $i \\in \\{ 1, 2, 3, 4 \\}$, $j \\in \\{ 1, 2\\}$. Note that *learning* in this context means calculating the values of the probabilities as relative frequencies.\n",
    "* Use the test data set (*data/test.txt*) for testing your classifier (i.e. predict the class labels of the $500$ test points using the given features and compare the predicted labels with the actual labels).\n",
    "\n",
    "The labelled training and test data are visualised below (the green and black points correspond to stable and unstable placements respectively):\n",
    "\n",
    "Training data | Test data\n",
    "- | - \n",
    "![alt](images/training_data.png) | ![alt](images/test_data.png)\n",
    "\n",
    "Implement your classifier below, plot your predicted labels (use the coordinates of the test points in *data/test_points.txt* for that purpose), and report the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) of your results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "[0.38, 0.62]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Training Bayes Classifier\"\"\"\n",
    "\n",
    "\n",
    "train=np.loadtxt('data/train.txt')\n",
    "\n",
    "P_F1, P_F2, P_F3, P_F4=[], [], [], []\n",
    "P_C0_F1, P_C0_F2, P_C0_F3, P_C0_F4=[], [], [], []\n",
    "P_C1_F1, P_C1_F2, P_C1_F3, P_C1_F4=[], [], [], []\n",
    "\n",
    "C_count=0\n",
    "F1_0_count, F1_1_count =0, 0\n",
    "F2_0_count, F2_1_count, F2_2_count =0, 0, 0 \n",
    "F3_0_count, F3_1_count, F3_2_count =0, 0, 0\n",
    "F4_0_count, F4_1_count, F4_2_count =0, 0, 0\n",
    "\n",
    "F1_0_C0_count, F1_1_C0_count =0, 0\n",
    "F2_0_C0_count, F2_1_C0_count, F2_2_C0_count =0, 0, 0 \n",
    "F3_0_C0_count, F3_1_C0_count, F3_2_C0_count =0, 0, 0\n",
    "F4_0_C0_count, F4_1_C0_count, F4_2_C0_count =0, 0, 0\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    if train[i][4]- 0<=1e-8:\n",
    "        C_count+=1\n",
    "        \n",
    "        #Calculating P(F_1|C0)\n",
    "        if train[i][0]- 0<=1e-8:\n",
    "            F1_0_C0_count+=1\n",
    "        elif train[i][0]- 1<=1e-8:\n",
    "            F1_1_C0_count+=1\n",
    "\n",
    "        #Calculating P(F_2|C0)\n",
    "        if train[i][1]- 0<=1e-8:\n",
    "            F2_0_C0_count+=1\n",
    "        elif train[i][1]- 1<=1e-8:\n",
    "            F2_1_C0_count+=1\n",
    "        elif train[i][1]- 2<=1e-8:\n",
    "            F2_2_C0_count+=1\n",
    "\n",
    "        #Calculating P(F_3|C0)\n",
    "        if train[i][2]- 0<=1e-8:\n",
    "            F3_0_C0_count+=1\n",
    "        elif train[i][2]- 1<=1e-8:\n",
    "            F3_1_C0_count+=1\n",
    "        elif train[i][2]- 2<=1e-8:\n",
    "            F3_2_C0_count+=1\n",
    "\n",
    "        #Calculating P(F_4|C0)\n",
    "        if train[i][3]- 0<=1e-8:\n",
    "            F4_0_C0_count+=1\n",
    "        elif train[i][3]- 1<=1e-8:\n",
    "            F4_1_C0_count+=1\n",
    "        elif train[i][3]- 2<=1e-8:\n",
    "            F4_2_C0_count+=1\n",
    "    \n",
    "    #Calculating P(F_1)\n",
    "    if train[i][0]- 0<=1e-8:\n",
    "        F1_0_count+=1\n",
    "    elif train[i][0]- 1<=1e-8:\n",
    "        F1_1_count+=1\n",
    "    \n",
    "    #Calculating P(F_2)\n",
    "    if train[i][1]- 0<=1e-8:\n",
    "        F2_0_count+=1\n",
    "    elif train[i][1]- 1<=1e-8:\n",
    "        F2_1_count+=1\n",
    "    elif train[i][1]- 2<=1e-8:\n",
    "        F2_2_count+=1\n",
    "    \n",
    "    #Calculating P(F_3)\n",
    "    if train[i][2]- 0<=1e-8:\n",
    "        F3_0_count+=1\n",
    "    elif train[i][2]- 1<=1e-8:\n",
    "        F3_1_count+=1\n",
    "    elif train[i][2]- 2<=1e-8:\n",
    "        F3_2_count+=1\n",
    "    \n",
    "    #Calculating P(F_4)\n",
    "    if train[i][3]- 0<=1e-8:\n",
    "        F4_0_count+=1\n",
    "    elif train[i][3]- 1<=1e-8:\n",
    "        F4_1_count+=1\n",
    "    elif train[i][3]- 2<=1e-8:\n",
    "        F4_2_count+=1\n",
    "\n",
    "P_C0=round(C_count/train.shape[0],2)\n",
    "P_C1=round(1-P_C0,2)\n",
    "            \n",
    "P_F1.append(round(F1_0_count/train.shape[0],2))\n",
    "P_F1.append(round(F1_1_count/train.shape[0],2))\n",
    "        \n",
    "P_F2.append(round(F2_0_count/train.shape[0],2))\n",
    "P_F2.append(round(F2_1_count/train.shape[0],2))\n",
    "P_F2.append(round(F2_2_count/train.shape[0],2))\n",
    "\n",
    "P_F3.append(round(F3_0_count/train.shape[0],2))\n",
    "P_F3.append(round(F3_1_count/train.shape[0],2))\n",
    "P_F3.append(round(F3_2_count/train.shape[0],2))\n",
    "\n",
    "P_F4.append(round(F4_0_count/train.shape[0],2))\n",
    "P_F4.append(round(F4_1_count/train.shape[0],2))\n",
    "P_F4.append(round(F4_2_count/train.shape[0],2))\n",
    "\"\"\"\"\"\"\n",
    "P_C0_F1.append(round(float(F1_0_C0_count)/C_count,2))\n",
    "P_C0_F1.append(round(float(F1_1_C0_count)/C_count,2))\n",
    "        \n",
    "P_C0_F2.append(round(F2_0_C0_count/C_count,2))\n",
    "P_C0_F2.append(round(F2_1_C0_count/C_count,2))\n",
    "P_C0_F2.append(round(F2_2_C0_count/C_count,2))\n",
    "\n",
    "P_C0_F3.append(round(F3_0_C0_count/C_count,2))\n",
    "P_C0_F3.append(round(F3_1_C0_count/C_count,2))\n",
    "P_C0_F3.append(round(F3_2_C0_count/train.shape[0],2))\n",
    "\n",
    "P_C0_F4.append(round(F4_0_C0_count/C_count,2))\n",
    "P_C0_F4.append(round(F4_1_C0_count/C_count,2))\n",
    "P_C0_F4.append(round(F4_2_C0_count/C_count,2))\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "P_C1_F1.append(round(1-P_C0_F1[0],2))\n",
    "P_C1_F1.append(round(1-P_C0_F1[1],2))\n",
    "        \n",
    "P_C1_F2.append(round(1-P_C0_F2[0],2))\n",
    "P_C1_F2.append(round(1-P_C0_F2[1],2))\n",
    "P_C1_F2.append(round(1-P_C0_F2[2],2))\n",
    "\n",
    "P_C1_F3.append(round(1-P_C0_F3[0],2))\n",
    "P_C1_F3.append(round(1-P_C0_F3[1],2))\n",
    "P_C1_F3.append(round(1-P_C0_F3[2],2))\n",
    "\n",
    "P_C1_F4.append(round(1-P_C0_F4[0],2))\n",
    "P_C1_F4.append(round(1-P_C0_F4[1],2))\n",
    "P_C1_F4.append(round(1-P_C0_F4[2],2))\n",
    "print(\"-\")     \n",
    "print P_C0_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prob_CF(f1,f2,f3,f4):\n",
    "    c=0\n",
    "    if (f1==0):\n",
    "        F1=P_F1[0]\n",
    "        FC1=P_C0_F1[0]\n",
    "    else:\n",
    "        F1=P_F1[1]\n",
    "        FC1=P_C0_F1[1]\n",
    "    \"\"\"\"\"\"\n",
    "    if (f2==0):\n",
    "        F2=P_F2[0]\n",
    "        FC2=P_C0_F2[0]\n",
    "    elif (f2==1):\n",
    "        F2=P_F2[1]\n",
    "        FC2=P_C0_F2[1]\n",
    "    else:\n",
    "        F2=P_F2[2]\n",
    "        FC2=P_C0_F2[2]\n",
    "    \"\"\"\"\"\"\n",
    "    if (f3==0):\n",
    "        F3=P_F3[0]\n",
    "        FC3=P_C0_F3[0]\n",
    "    elif (f3==1):\n",
    "        F3=P_F3[1]\n",
    "        FC3=P_C0_F3[1]\n",
    "    else:\n",
    "        F3=P_F3[2]\n",
    "        FC3=P_C0_F3[2]\n",
    "    \"\"\"\"\"\"\n",
    "    if (f4==0):\n",
    "        F4=P_F4[0]\n",
    "        FC4=P_C0_F4[0]\n",
    "    elif (f4==1):\n",
    "        F4=P_F4[1]\n",
    "        FC4=P_C0_F4[1]\n",
    "    else:\n",
    "        F4=P_F4[2]\n",
    "        FC4=P_C0_F4[2]\n",
    "   \n",
    "    P_C_F=P_C0*FC1*FC2*FC3*FC4/(F1*F2*F3*F4)\n",
    "    if (P_C_F<0.5):\n",
    "        c=1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6e482b02c06b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mC_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_CF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mC_actual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mC_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0mC_actual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-da6e0889dca2>\u001b[0m in \u001b[0;36mprob_CF\u001b[0;34m(f1, f2, f3, f4)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mFC4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP_C0_F4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mP_C_F\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP_C0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mFC3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mFC4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mF2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mF3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mF4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mP_C_F\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "test=np.loadtxt('data/test.txt')\n",
    "test_points=np.loadtxt('data/test_points.txt')\n",
    "\n",
    "C_predicted=np.zeros(test.shape[0])\n",
    "C_actual=np.zeros(test.shape[0])\n",
    "\n",
    "plt.figure(1,figsize=(8, 6))\n",
    "plt.title('Predicted')\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.title('Actual')\n",
    "cnt=0\n",
    "for i in range(test.shape[0]):\n",
    "    f1,f2,f3,f4=test[i][0],test[i][1],test[i][2],test[i][3]\n",
    "    C_predicted[i]=prob_CF(f1,f2,f3,f4)\n",
    "    C_actual[i]=test[i][4]\n",
    "    if (C_predicted[i]!= C_actual[i]):\n",
    "        #print (i)\n",
    "        cnt+=1\n",
    "    if (C_predicted[i]-0<1e-8):\n",
    "        col='black'\n",
    "    else:\n",
    "        col='green'\n",
    "    if (C_actual[i]-0<1e-8):\n",
    "        col2='black'\n",
    "    else:\n",
    "        col2='green'\n",
    "    plt.figure(1)\n",
    "    plt.scatter(test_points[i][0],test_points[i][1], color=col)\n",
    "    plt.figure(2)\n",
    "    plt.scatter(test_points[i][0],test_points[i][1], color=col2)\n",
    "\n",
    "print (\"Number of unmatched predictions: \",cnt)\n",
    "plt.figure(1)\n",
    "img = plt.imread(\"images/configuration.png\")\n",
    "plt.imshow(img, extent=[-1, 6, -1, 6])\n",
    "plt.figure(2)\n",
    "img = plt.imread(\"images/configuration.png\")\n",
    "plt.imshow(img, extent=[-1, 6, -1, 6])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 2: Total Probability and Bayes' Rule [40 points]\n",
    "\n",
    "Let's suppose that there is a box of tools in the RoboCup@Work lab which has $20$ wrenches, $50$ screwdrivers, and $30$ pairs of pliers, such that we let one of our youBots pick up a single tool from the box at random. From historical data, we know that there is a $0.2$ probability that the robot will drop a wrench, a $0.1$ probability that it will drop a screwdriver, and a $0.3$ probability that it will drop a pair of pliers; in other words,\n",
    "\n",
    "$P(d|w) = 0.2 \\hspace{2cm} P(d|s) = 0.1 \\hspace{2cm} P(d|p) = 0.3$\n",
    "\n",
    "where $d$ is the event in which the youBot drops a tool, while $w, s$, and $p$ are the events in which the robot picks up a wrench, a screwdriver, and a pair of pliers respectively. If the youBot drops a tool, a human operator puts the tool back into the gripper and the robot continues its operation.\n",
    "\n",
    "1. What is the probability that the robot drops a tool?\n",
    "2. If we know that the robot has dropped a tool, what is the probability that it had picked up a pair of pliers?\n",
    "3. Show that the probability that the youBot had picked up a screwdriver given two observed drops is equal to $$P(s|d_1,d_2) = \\frac{P(d_2|s)P(s|d_1)}{P(d_2|s)P(s|d_1) + P(d_2|w)P(w|d_1) + P(d_2|p)P(p|d_1)}$$ which means that, if we want to calculate the probability that the robot had picked up a screwdriver given two drops, we first need to calculate the posterior probabilities that the robot had picked up any of the tools after dropping the tool for the first time (which can in fact be seen as updating the prior probabilities in a recursive manner).\n",
    "4. Generalise the above result to the case in which we observe $n$ drops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### write your answers here ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
